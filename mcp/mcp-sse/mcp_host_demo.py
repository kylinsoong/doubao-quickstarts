import asyncio
import json
import re
import os
from byteLLM import ByteLLM
from mcp_client_demo import MCPClient

api_key = os.environ.get("ARK_API_KEY")
model_id = os.environ.get("ARK_API_ENGPOINT_ID")



def extract_json_from_response_content(content):
    """
    æå– LLM è¿”å›çš„contentå†…å®¹ï¼Œè§£æJSONç»“æ„ï¼Œæ‹¿å‡ºtoolä¿¡æ¯
    """
    # å¦‚æœå·²ç»æ˜¯ dictï¼Œç›´æ¥è¿”å›
    if isinstance(content, dict):
        return content

    # æ¸…é™¤ markdown ```json ``` åŒ…è£¹
    if isinstance(content, str):
        content = re.sub(r"^```(?:json)?|```$", "", content.strip(), flags=re.IGNORECASE).strip()

    # æœ€å¤šå°è¯• 3 å±‚ json.loads è§£ç 
    for _ in range(3):
        try:
            parsed = json.loads(content)
            if isinstance(parsed, dict):
                return parsed
            else:
                content = parsed  # å¦‚æœè§£å‡ºæ¥è¿˜æ˜¯ strï¼Œç»§ç»­ä¸‹ä¸€å±‚
        except Exception:
            break

    # å¦‚æœæœ€ç»ˆä¸æ˜¯ dictï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²ï¼ˆè¡¨ç¤ºæ˜¯æ™®é€šç­”å¤ï¼‰
    return content


llm = ByteLLM(model=model_id, api_key=api_key)


async def main():
    # === åˆå§‹åŒ– MCP å®¢æˆ·ç«¯ ===
    client = MCPClient()
    await client.__aenter__()

    tools = await client.list_tools()

    # resources = await client.list_resources()
    tool_names = [t.name for t in tools.tools]

    tool_descriptions = "\n".join(f"- å·¥å…·åç§°ï¼š{t.name} \nå·¥å…·æè¿°ï¼š{t.description} \nå…¥å‚è¦æ±‚ï¼š{t.inputSchema}" for t in tools.tools)
    # resource_descriptions = "\n".join(f"- {r.uri}" for r in resources.resources)

    while True:
        # === Step 1. ç”¨æˆ· â†’ MCP hostï¼šæå‡ºé—®é¢˜ ===
        user_input = input("\nè¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰ï¼š\n> ")
        if user_input.lower() in ("exit", "é€€å‡º"):
            break

        system_prompt = f'''
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œæ‹¥æœ‰ä»¥ä¸‹å·¥å…·å¯ä»¥è°ƒç”¨ï¼š

# å·¥å…·åˆ—è¡¨
{tool_descriptions}

# æ³¨æ„äº‹é¡¹
1ï¼Œè¯·ä¼˜å…ˆåˆ¤æ–­æ˜¯å¦ä½¿ç”¨ã€å·¥å…·åˆ—è¡¨ã€‘ä¸­çš„å·¥å…·ï¼Œå¦‚ä½¿ç”¨å·¥å…·ï¼Œåˆ™è¿”å›æ ¼å¼ä¸ºï¼š
    {{"tool_calls": 
        [
            {{"name": "xxx", "arguments": {{"xxx": "xxx"}}}}
        ]
    }}
    å…¶ä¸­argumentsæ ¼å¼åŠ¡å¿…ä¸å·¥å…·çš„ã€å…¥å‚è¦æ±‚ã€‘ä¿æŒä¸€è‡´
2ï¼Œä¸è¦æé€ ä¸å­˜åœ¨çš„å·¥å…·
3ï¼Œå¦‚æ— éœ€ä½¿ç”¨å·¥å…·ï¼Œåˆ™ç»“åˆä¸Šä¸‹æ–‡å›å¤ç”¨æˆ·æœ€ç»ˆç»“æœ
4ï¼Œå¦‚æœä½¿ç”¨å·¥å…·ï¼Œåªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦è§£é‡Š
'''

        # === æ„é€  LLM ä¸Šä¸‹æ–‡æ¶ˆæ¯ ===
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_input}
        ]

        final_reply = ""

        # === å¾ªç¯å¤„ç† tool_callsï¼Œç›´åˆ° LLM ç»™å‡ºæœ€ç»ˆ content ä¸ºæ­¢ ===
        while True:
            # === Step 1. MCP host â†’ LLMï¼šè½¬å‘ä¸Šä¸‹æ–‡ ===
            print("******************* new round *****************")
            response = llm.generate(messages)

            if hasattr(response, "reasoning_content"):
                print(f"\nï¼ï¼ï¼LLM æ€è€ƒï¼š\n{response.reasoning_content}")


            messages.append({
                "role":"assistant",
                "content": response.content
            })

            # === Step 2. è§£æ JSON æ ¼å¼å›å¤ï¼ˆæˆ–æ™®é€šå­—ç¬¦ä¸²ï¼‰ ===
            parsed = extract_json_from_response_content(response.content)

            # === å¦‚æœæ˜¯æ™®é€šè‡ªç„¶è¯­è¨€å­—ç¬¦ä¸²ï¼Œè¯´æ˜ LLM å·²ç›´æ¥ç­”å¤ç”¨æˆ· ===
            if isinstance(parsed, str):
                final_reply = parsed
                print("*********************è¿™é‡Œè¡¨æ˜æ¨¡å‹ä¸å†è°ƒç”¨å·¥å…·ï¼Œå‡†å¤‡æœ€ç»ˆè¿”å›ç»“æœ*************************")
                break

            # === å¦‚æœæ˜¯å­—å…¸ï¼Œåˆ¤æ–­æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨ ===
            tool_calls = parsed.get("tool_calls")
            if not tool_calls:
                # LLM ç»™å‡ºæ™®é€šç­”å¤ç»“æ„ï¼ˆå¸¦ content å­—æ®µï¼‰
                final_reply = parsed
                break

            # === éå† LLM è¯·æ±‚çš„å·¥å…·è°ƒç”¨åˆ—è¡¨ ===
            for tool_call in tool_calls:
                # === Step 3. LLM â†’ MCPå®¢æˆ·ç«¯ï¼šè¯·æ±‚ä½¿ç”¨å·¥å…· ===
                tool_name = tool_call["name"]
                arguments = tool_call["arguments"]

                if tool_name not in tool_names:
                    raise ValueError(f"å·¥å…· {tool_name} æœªæ³¨å†Œ")

                # === Step 4. MCPå®¢æˆ·ç«¯ â†’ MCPæœåŠ¡å™¨ï¼šè°ƒç”¨å·¥å…· ===
                print(f"\nï¼ï¼ï¼è°ƒç”¨å·¥å…· {tool_name} å‚æ•°: {arguments}")
                result = await client.call_tool(tool_name, arguments)

                # === Step 5. MCPæœåŠ¡å™¨ â†’ MCPå®¢æˆ·ç«¯ï¼šè¿”å›ç»“æœ ===
                tool_output = result.content[0].text
                print(f"\nï¼ï¼ï¼å·¥å…· {tool_name} è¿”å›ï¼š{tool_output}")

                messages.append({
                    "role": "user",
                    "content":  f'è°ƒç”¨å·¥å…·ï¼š{tool_name} \nè°ƒç”¨ç»“æœï¼š{tool_output}'
                })

        # === Step 6. MCPä¸»æœº â†’ ç”¨æˆ·ï¼šæœ€ç»ˆç»“æœç­”å¤ ===
        print(f"\nğŸ¯ æœ€ç»ˆç­”å¤ï¼š{final_reply}")

    await client.__aexit__(None, None, None)


if __name__ == "__main__":
    asyncio.run(main())
