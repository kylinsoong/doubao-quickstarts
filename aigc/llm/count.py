import os
import hashlib
from collections import defaultdict

def hash_file(path):
    with open(path, 'rb') as f:
        return hashlib.md5(f.read()).hexdigest()

folder = 'md1'  
hash_map = defaultdict(list)

# éå†æ‰€æœ‰ .md æ–‡ä»¶
for file in os.listdir(folder):
    if file.endswith('.md'):
        file_path = os.path.join(folder, file)
        file_hash = hash_file(file_path)
        hash_map[file_hash].append(file)

# è¾“å‡ºé‡å¤æ–‡ä»¶ç»„
total_files = 0
duplicate_groups = 0
for files in hash_map.values():
    total_files += len(files)
    if len(files) > 1:
        duplicate_groups += 1
        print(f"ğŸ” Duplicate group ({len(files)} files): {files}")

print(f"\nğŸ“Š Total files: {total_files}")
print(f"â™»ï¸  Duplicate groups: {duplicate_groups}")
print(f"ğŸ“ˆ  Duplicate rate: {round((total_files - len(hash_map)) / total_files * 100, 2)}%")

