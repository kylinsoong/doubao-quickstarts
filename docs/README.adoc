= AI
:toc: manual

= GPU 

== GPU 计算精度

在使用 GPU 进行深度学习训练或推理时，计算精度（Precision）是一个关键因素。常见的精度格式包括FP32（32 位浮点）、FP16（16 位浮点）、BF16（bfloat16）、INT8（8 位整型）、INT4（4位整型）等。

计算精度和资源开销基本原则：

1. 精度越高（如 FP32），数值表达能力更强，计算结果更准确，但同时也会带来更高的计算和存储成本，包括更大的模型体积、更高的内存占用和更慢的计算速度。
2. 精度越低（如 FP16 或 INT8），虽然可能略微影响模型的精度表现，但可以大幅减少模型尺寸和推理过程中的内存使用，有助于提升 GPU 的计算效率和整体性能。

[cols="2,5a"]
.*常见简称*
|===
|NAME |NOTES

|TFLOPS
|每秒执行1万亿次浮点运算次数。（F表示Float浮点）

|TOPS
|每秒执行1万亿次运算次数

|FP
|代表浮点运算数据格式，包括双精度（FP64）、单精度（FP32）、半精度（FP16）以及FP8 等

|INT
|代表整数格式，包括INT8、INT4等。后面的数字位数越高，意味着精度越高，能够支持的运算复杂程度就越高，适配场景越广

|FP32
|
* 也叫做 float32，两种叫法是完全一样，全称是Single-precision floating-point(单精度浮点数)
* 用于`训练`

|BF16
|
* 也叫做BFLOAT16 (这是最常叫法)，全称brain floating point，用16位二进制来表示的，Google Brain开发
* 用于`训练`、`推理`

|FP16
|
* 也叫float16，全称是Half-precision floating-point(半精度浮点数)
* 用于`训练`、`推理`。

|FP8
|8位精度, 用于`训练`、`推理`

|INT8
|量化精度，用于`推理`

|INT4
|不具备生产能力

|===

== 常见卡型参数

|===
.*常见卡型参数*
|参数|T4 |A100 |A800 |L20 |H20 96G |H20 141G |H200 |910B |910C(A+K) |910C(X+X)

|上市时间
|2018-09
|2020-09
|2022-11
|2023-11
|2023-11
|2025
|2024
|2023
|2025
|2025

|FP64(TFLOPS)
|-
|9.7
|9.7
|-
|-
|-
|34
|-
|-
|-

|FP64 Tensor Core(TFLOPS)
|-
|19.5
|19.5
|-
|-
|-
|67
|-
|-
|-

|FP32(TFLOPS)
|8.1
|19.5
|19.5
|59.6
|39.5
|39.5
|67
|96
|192
|192

|TF32 Tensor Core(TFLOPS)
|-
|156/312*
|156/312*
|59.8
|74
|74
|494/989*
|-
|-
|-

|FP16(TFLOPS)
|16.2
|-
|-
|-
|-
|-
|-
|376
|752
|752

|BFloat16 Tensor Core(TFLOPS)
|-
|312/624*
|312/624*
|119.5
|148
|148
|989/1979*
|-
|-
|-

|FP16 Tensor Core(TFLOPS)
|65
|312/624*
|312/624*
|119.5
|148
|148
|989/1979*
|-
|-
|-

|FP8 Tensor Core(TFLOPS)
|-
|-
|-
|238
|296
|296
|1979/3958*
|-
|-
|-

|INT8 Tensor Core(TOPS)
|130
|624/1248*
|624/1248*
|239
|296
|296
|1979/3958*
|-
|-
|-

|INT4(TOPS)
|260
|-
|-
|-
|-
|-
|-
|-
|-
|-

|GPU显存(GB)
|16
|80
|80
|48
|96
|141
|141
|96
|128
|128

|GPU显存类型
|GDDR6
|HBM2e
|HBM2e
|GDDR6
|HBM3
|HBM3e
|HBM3e
|HBM2e
|HBM2e
|HBM2e

|GPU显存带宽(GB/s)
|300
|2039
|2039
|864
|4000
|4800
|4800
|1600
|3200
|3200

|===



= 推理基础

== 推理常见问题
